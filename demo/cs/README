demo/cs:

	cxor.proj.gz		stochastic completion-xor
	better_cxor/		a better version of cxor contributed
				by Karl Pfleger, see dir for docs

This example shows how a stochastic network can be trained to produce
a target distribution of patterns over a set of visible units where
each pattern is the input/output states of the XOR problem.  There are
no real input units, only outputs, and only a single training event.

The four patterns in the single event are put in a
'ProbPatternSpec_Group', which allows them to be chosen with a
specified probability to be presented as training examples to the
network.  The network will then match this probability distribution in
its own random generation of the output patterns based on the
intrinsic noise that is present during settling.

In this example, a TIG stat records the Total Information Gain, which
is a measure of how far apart the actual and desired probability
distributions are.  Also, a Targ stat records the percentage of time
spent in one of the target patterns.  These are shown epoch-by-epoch
on the lower left-hand graph.

The basic information that these two statisitics work from is computed
by the Dist Stat, which records the percentage of time that the network
settles into each of the target patterns.  These percentages are
recorded in the graph on the lower right---each of the lines should
converge towards .25 percent, which is what the TIG stat is measuring.


	figgr.proj.gz		figure-ground constraint satisfaction
	figgr_objreps.wts	object weights for figure ground
	figgr_setup_network.css	script that builds network upon loading

This simulation is an example of large-scale constraint satisfaction
in the service of solving the figure-ground organization problem.  The
example is taken from the work described in Vecera & O'Reilly,
submitted, which is based on earlier models by Sejnowski, Hinton and
colleagues.  The network is presented an ambiguous contour on the
boundary units, and it tries to settle into a shape representation on
the figure-ground units that fills in one of the two halves of the
ambiguous boundary as figure.  There is a top-down cue in the form of
object representations that have weights encoding two 'familiar'
objects.  The network is more likely to call the region that matches
the familiar object 'figure' on the basis of these top-down inputs.

The simulation uses the TesselPrjnSpec extensively to set up the
hand-wired connectivity between the figure-ground and boundary units.
Further, it uses a setup_network script that is auto-run at the start
to actually build and connect the network---it is much faster to save
just the layers and projections of a large network like this and have
it build itself upon startup than it is to save a pre-built network.

In running the network, you can track the goodness on the lower-right
hand graph, which shows how well the network is settling into a 'good'
solution to the boundary and object constraints.  You can see by
trying NewInit that sometimes it will not settle into a very
interpretable figure representation--this will have a low goodness
value.  The NetView is set to update after settling, but you can
periodically press the Update button to see how things are going.
Notice that the activations are very noisy at the start, and then
become less so as settling proceeds--this is because an annealing
schedule was used to help the network avoid local minima.  You can see
this schedule in the unit specs.

