<HTML>
<HEAD>
<!-- This HTML file has been created by texi2html 1.51pdp
     from ../pdp-user.texi on 2 May 2003 -->

<TITLE>The PDP++ Software Users Manual - leabra</TITLE>
</HEAD>
<BODY>
<A HREF="pdp-user_1.html"><IMG SRC="icons/top.gif"><ALT="first,"></A><A HREF="pdp-user_234.html"><IMG SRC="icons/prev.gif"><ALT="previous,"></A><A HREF="pdp-user_236.html"><IMG SRC="icons/next.gif"><ALT="next,"></A><A HREF="pdp-user_1.html"><IMG SRC="icons/up.gif"><ALT="up,"></A><A HREF="pdp-user_toc.html"><IMG SRC="icons/toc.gif"><ALT="table of contents"></A><HR>


<H1><A NAME="IDX1451" HREF="pdp-user_toc.html">17  Leabra</A></H1>

<P>
<A NAME="IDX1452"></A>

</P>
<P>
Leabra stands for "Local, Error-driven and Associative, Biologically
Realistic Algorithm", and it implements a balance between Hebbian and
error-driven learning on top of a biologically-based point-neuron
activation function with inhibitory competition dynamics (either via
inhibitory interneurons or a fast k-Winners-Take-All approximation
thereof).  Extensive documentation is available from the book:
"Computational Explorations in Cognitive Neuroscience: Understanding
the Mind by Simulating the Brain", O'Reilly and Munakata, 2000,
Cambridge, MA: MIT Press.  For more information, see the website:
<CODE>http://psych.colorado.edu/~oreilly/comp_ex_cog_neuro.html</CODE>.

</P>
<P>
Hebbian learning is performed using conditional principal
components analysis (CPCA) algorithm with correction factor for
sparse expected activity levels.

</P>
<P>
Error driven learning is performed using GeneRec, which is a
generalization of the Recirculation algorithm, and approximates
Almeida-Pineda recurrent backprop.  The symmetric, midpoint version
of GeneRec is used, which is equivalent to the contrastive Hebbian
learning algorithm (CHL).  See <CITE>O'Reilly (1996; Neural Computation)</CITE>
for more details.

</P>
<P>
The activation function is a point-neuron approximation with both
discrete spiking and continuous rate-code output.

</P>
<P>
Layer or unit-group level inhibition can be computed directly using
a k-winners-take-all (KWTA) function, producing sparse distributed
representations, or via inihibitory interneurons.

</P>
<P>
The net input is computed as an average, not a sum, over
connections, based on normalized, sigmoidaly transformed weight
values, which are subject to scaling on a connection-group level to
alter relative contributions.  Automatic scaling is performed to
compensate for differences in expected activity level in the
different projections.

</P>
<P>
Weights are subject to a contrast enhancement function, which
compensates for the soft (exponential) weight bounding that keeps
weights within the normalized 0-1 range.  Contrast enhancement is
important for enhancing the selectivity of self-organizing learning,
and generally results in faster learning with better overall results.
Learning operates on the underlying internal linear weight value,
which is computed from the nonlinear (sigmoidal) weight value prior to
making weight changes, and is then converted back.  The linear weight
is always stored as a negative value, so that shared weights or
multiple weight updates do not try to linearize the already-linear
value.  The learning rules have been updated to assume that wt is
negative (and linear).

</P>
<P>
There are various extensions to the algorithm that implement things
like reinforcement learning (temporal differences), simple recurrent
network (SRN) context layers, and combinations thereof (including an
experimental versions of a complex temporal learning mechanism based on
the prefrontal cortex and basal ganglia).  Other extensions include a
variety of options for the activation and inhibition functions, self
regulation (accommodation and hysteresis, and activity regulation for
preventing overactive and underactive units), synaptic depression, and
various optional learning mechanisms and means of adapting
parameters.  These features are off by default but do appear in some
of the edit dialogs -- any change from default parameters should be
evident in the edit dialogs.

</P>

<UL>
<LI><A HREF="pdp-user_236.html">1                     Overview of the Leabra Algorithm</A>
<LI><A HREF="pdp-user_237.html">2                      Leabra Connection Specifications</A>
<LI><A HREF="pdp-user_238.html">3                     Leabra Unit Specifications</A>
<LI><A HREF="pdp-user_239.html">4                    Leabra Layer Specifications</A>
<LI><A HREF="pdp-user_240.html">5                     Leabra Processes</A>
<LI><A HREF="pdp-user_241.html">6                    Leabra Statistics</A>
<LI><A HREF="pdp-user_244.html">7                     Leabra Defaults</A>
<LI><A HREF="pdp-user_245.html">8                     Leabra Misc Special Classes</A>
<LI><A HREF="pdp-user_246.html">9                     Leabra Implementation Details</A>
</UL>

<P><HR><P>
<A HREF="pdp-user_1.html"><IMG SRC="icons/top.gif"><ALT="first,"></A><A HREF="pdp-user_234.html"><IMG SRC="icons/prev.gif"><ALT="previous,"></A><A HREF="pdp-user_236.html"><IMG SRC="icons/next.gif"><ALT="next,"></A><A HREF="pdp-user_1.html"><IMG SRC="icons/up.gif"><ALT="up,"></A><A HREF="pdp-user_toc.html"><IMG SRC="icons/toc.gif"><ALT="table of contents"></A></BODY>
</HTML>
