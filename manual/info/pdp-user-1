This is pdp-user, produced by makeinfo version 4.1 from pdp-user.texi.


File: pdp-user,  Node: Top,  Next: copyright,  Prev: (dir),  Up: (dir)

Welcome to the PDP++ Software Users Manual
******************************************

   Chadley K. Dawson, Randall C. O'Reilly, and James L. McClelland.

   This describes version 3.0 of the PDP++ neural network simulation
software.

   Manual revision date: 2 May 2003.

* Menu:

* intro::               Introduction to the PDP++ Software
* inst::                Installation Guide
* over::                Conceptual Overview
* tut::                 Tutorial Introduction (using Bp)
* how::                 How-to Guide and the Wizard
* gui::                 Guide to the Graphical User Interface (GUI)
* css::                 Guide to the Script Language (CSS)
* obj::                 Object Basics and Basic Objects
* proj::                Projects (and Defaults, Scripts, SelectEdit)
* net::                 Networks (Layers, Units, etc)
* env::                 Environments, Events, and Patterns
* proc::                Processes and Statistics
* log::                 Logs and Graphs
* bp::                  Backpropagation
* cs::                  Constraint Satisfaction
* so::                  Self-organizing Learning
* leabra::              Leabra
* prog::                Programming in PDP++
* copyright::           Appendix A: Copyright Information
* conceptindex::        Index of Key Concepts
* typeindex::           Index of Class Types
* varindex::            Index of Variables
* funcindex::           Index of Functions


File: pdp-user,  Node: intro,  Next: inst,  Prev: copyright,  Up: Top

Introduction to the PDP++ Software
**********************************

   As the field of connectionist modeling has grown, so has the need
for a comprehensive simulation environment for the development and
testing of connectionist models.  Our goal in developing PDP++ has
been to integrate several powerful software development and user
interface tools into a general purpose simulation environment that is
both user friendly and user extensible.  The simulator is built in
the C++ programming language, and incorporates a state of the art
script interpreter with the full expressive power of C++.  The
graphical user interface is built with the Interviews toolkit, and
allows full access to the data structures and processing modules out
of which the simulator is built.  We have constructed several useful
graphical modules for easy interaction with the structure and the
contents of neural networks, and we've made it possible to change and
adapt many things.  At the programming level, we have set things up
in such a way as to make user extensions as painless as possible.
The programmer creates new C++ objects, which might be new kinds of
units or new kinds of processes; once compiled and linked into the
simulator, these new objects can then be accessed and used like any
other.

   The system is very powerful, and as with all powerful things, one
needs to learn how to handle it to get the most out of it.  But the
system is also engineered for ease of use by the beginner.  Classic
programs (such as backpropagation) and classic example simulations
(like learning XOR) have been set up so the user can run them and
explore their behavior with a few key strokes and mouse clicks.  It's
easy to set up new examples and modify various parameters and explore
the effects.  There are libraries of different unit and process types,
so it is easy for you to put together novel simulations on your own.

   This manual provides a complete overview of the PDP++ system,
starting from the simplest kind of uses and progressing to a full
description of all of the features so you can get the most out of the
system.  We strongly recommend reading the *Note over:: chapter in
its entirety, and then stepping through the *Note tut:: chapter to
get some hands-on familiarity with the system.  If the software is
not yet installed on your system, you must follow the instructions in
the *Note inst:: chapter to install it.  The remainder of the manual
can be used as a reference as needed, or read through for a deeper
understanding of how things really work.

   Before you'll be able to use this system, or even understand this
manual, you will need some background.  First, you'll need to have a
working understanding of the Unix operating system, the X11 window
system, and your window manager (Motif, OpenWindows, or whatever you
use).  You'll probably also need to know something about
connectionist/neural network simulation models.  For general
background, consider Chapters 1-4 of Parallel Distributed Processing,
Volume 1 (Rumelhart, McClelland, and the PDP Research Group, 1986).
While section BP of this tutorial presents back propagation as a
procedure, it does not fully motivate it or discuss the reasons for
its importance; for this we refer you to Chapter 8 of PDP Volume 1, to
Chapter 4 of Explorations in Parallel Distributed Processing
(McClelland and Rumelhart, 1988) or to Rumelhart, Hinton and Williams
(1986).

   New for 2.0: The PDP++ software provides the basis for a large
number of cognitive-neuroscience oriented simulations in the new book
"Computational Explorations in Cognitive Neuroscience: Understanding
the Mind by Simulating the Brain", by O'Reilly and Munakata, MIT
Press, 2000 (September).  These simulations use the `leabra++'
algorithm, which is now provided with the PDP++ software in the
`src/leabra' directory.  The simulations can be downloaded for free
from either the MIT Press website (`http://www.mitpress.com' - search
for the book by title or authors) or from a link off of O'Reilly's we
page at `http://psych.colorado.edu/~oreilly'.  Of course, to get the
most out of them, it is recommended that you purchase the book ;).

   A mailing list is available for users of PDP++ to share ideas and
help each other solve problems.  This list will be monitored by the
authors of the PDP++ software, but ABSOLUTELY NO GUARANTEE or even
SUGGESTION of technical assistance is implied by the existence of
this list.  We have limited resources and are making the software
available as a public service.  Please respect this by not asking for
help from the authors directly.  To be added or removed from the
mailing list, and/or to view archives of previous messages, see the
instructions at the website,
`http://psych.colorado.edu/~oreilly/PDP++/PDP++.html'.

   For bug reports (which we will accumulate and fix), write to
`pdp++bugreport@cnbc.cmu.edu'.  Again, there is no guarantee that
your bug will be fixed.  However, sending in a complete description
of the actions that lead to the bug so we can reproduce it is
essential for anything to be done to fix it.  Also see the `TODO'
files in the top level and various sub-directories (ta and css) for
things we are planning to do.


File: pdp-user,  Node: inst,  Next: over,  Prev: intro,  Up: Top

Installation Guide
******************

   This chapter provides a guide to installing the PDP++ software.
There are two basic forms in which the software is distributed -- the
executable files only for use by an "end user", and the complete
source code, for use by a "programmer" who will be compiling new
additions to PDP++.  We will refer to the executable files only
distribution as the "end user's version", and the source code
distribution as the "programmer's version", where the version refers
to the manner in which the software is distributed, not to the
software itself (it's all the same code).

   For most systems, the end user's distribution is obtained in two
parts, a tar file containing pre-compiled binaries for a particular
system, and another which contains the manual, demos, default files,
and other miscellaneous things.  This is for the end user who will
not need to compile new versions of the software to add new
functionality to it.  The relevant tar files are:
       pdp++_version_bin_CPU.tar.gz
       pdp++_version_ext.tar.gz
   where VERSION is the version number of the software release, and
CPU is the cpu-type of the system you will be running on (see below).

   For LINUX systems under RedHat or compatible distributions, an rpm
version (with the `.rpm' extension) is available.

   For MS Windows, a standard `setup' install file (with the `.exe'
extension) is available.

   For Mac OS-X, a standard "package" file (with the `.pkg.sit'
extension) is available.

   The currently supported CPU types (listed in rough order of level
of support) are:
`LINUX'
     An Intel 386-Pentium machine running a modern glibc version of
     Linux (e.g., RedHat 7+).

`CYGWIN'
     An Intel 386-Pentium machine running the Windows operating
     system (using the Cygnus CygWin system as a compilation
     environment).

`DARWIN'
     A Mac running OS-X (aka Darwin), which is based on FreeBSD.
     Binaries are for the power PC (PPC) architecture.

`SUN4'
     A SUN sparc-station system running a modern 5.x Solaris version
     of the operating system.

`SGI'
     A Silicon Graphics workstation running a recent Irix 6.x release.

`HP800'
     A Hewlett Packard workstation running HP-UX version 10.x.

`IBMaix'
     An IBM RS/6000 machine running AIX v4.1.4 (4.1.x shouldwork)

   If you don't have one of these machines, then you will have to
compile the software from the source code using the programmers
distribution.

   The programmer's distribution is contained in one tar file that
contains the source code along with the manual and other supporting
files:
       pdp++_version_src.tar.gz

   These and any other files mentioned below can be obtained from our
anonymous FTP servers:
     CMU FTP Site: 	        ftp://cnbc.cmu.edu/pub/pdp++/
     Colorado FTP Site: 	ftp://grey.colorado.edu/pub/oreilly/pdp++/
     European (UK) Mirror:   ftp://unix.hensa.ac.uk/mirrors/pdp++/
   The Colorado site is updated most frequently.

* Menu:

* inst-users::                  Installing the End User's Version
* inst-prog::                   Installing the Programmers Version


File: pdp-user,  Node: inst-users,  Next: inst-prog,  Prev: inst,  Up: inst

Installing the End User's Version
=================================

   After downloading the two tar files, `pdp++_version_bin_CPU.tar.gz'
and `pdp++_version_ext.tar.gz', you need to decide where to locate
the files.  It is recommended that you put them in
`/usr/local/pdp++', but they can be put anywhere.  However, the
`PDPDIR' environmental variable must then be set for all users to the
location it is actually installed in.  In addition if your CPU
supports shared libraries (all unix versions, including LINUX, IBMaix,
SUN4, HP800, SGI, but not DARWIN), you will need to insure that the
`LD_LIBRARY_PATH' environment variable includes the path
`PDPDIR/lib/CPU' where PDPDIR is the location of the pdp++
distribution, and CPU is your system type as described above (more
details on this below).  The following will assume that you are
installing in `/usr/local/pdp++'.

   Note: all of the PDP++ software is distributed in the gnu `gzip'
format, and it also uses gzip to automatically compress and decompress
the network, project, and environment files so that they take up less
space on your disk.  Thus, your system must have `gzip' installed
before proceeding.  It can be obtained from the GNU ftp server
(`gnudist.gnu.org') or one of its mirrors, and is typically installed
on most modern systems anyway.

   Go to the `/usr/local' directory, and issue the following command:
       gzip -dc <tarfile> | tar -xf -
   or, on Linux or other systems having a gnutar program
       tar -xzf <tarfile>
   where <TARFILE> is the name of the tar archive file.  Note that
the tar files will create the pdp++ directory, or load into it if it
already exists.  Thus, if you have an old version of the software, be
sure to rename its directory something else before loading the new
files.

   LINUX users: There is a special `.rpm' file that will install the
LINUX binaries and ext tar contents, including making links to the
binaries in /usr/local/bin, and installing the libIV.so library (and
links) in /usr/local/lib, and all of the ext-tra stuff in
/usr/local/pdp++.  To install this file, you need to be super-user,
and then execute the following command:
       rpm -Uvh pdp++-binext-VERSION.i386.rpm
   Note that the PDP++ specific libraires are still installed in
PDPDIR/lib/LINUX, so you still need to set the LD_LIBRARY_PATH to
include this path.

   Windows users (CYGWIN): There is a special `.exe' file that is an
auto-installing executable distribution of both the bin and ext tar
files described above.  This should be used to install under windows.
If you should install it in a location other than the default
`C:\PDP++' directory, you should add a `set PDPDIR=path' in the
`C:\autoexec.bat' file.

   OS-X users: There is a special `.pkg.sit' file that is an
auto-installing package file distribution of both the bin and ext tar
files described above.

   All further references to file names, unless otherwise stated,
assume that you are in the PDPDIR directory (e.g.,
`/usr/local/pdp++').

   The files will get loaded into the following directories:

`bin'
     binaries (executable files) go here

`config'
     configuration (for Makefile) and some standard init files are
     found here

`css'
     contains include files for commonly-used css scripts and some
     additional documentation, plus some demo script files

`defaults'
     contains default configuration files for the various executables
     (see the manual for more information).

`demo'
     contains demonstrations of various aspects of the PDP++ software.

`manual'
     contains the manual, which is in texinfo format	and has been made
     into a .ps, emacs .info, and html files.

`src'
     contains the source code for the software.

`lib'
     libraries (for dynamically linked executables) go here

`interviews/lib'
     InterViews toolkit libraries (for dynamically linked
     executables) go here.

   The binaries will get unloaded into `bin/CPU', where `CPU' is the
system name as described above.  The binaries are:

`bp++'
     The backpropagation executable (*note bp::).

`cs++'
     The constraint satisfaction executable (*note cs::).

`so++'
     The self-organizing learning executable (*note so::).

`bpso++'
     A combination of backpropagation and self-organization
     algorithms, so hybrid networks can be built.

`leabra++'
     The Leabra algorithm developed by O'Reilly, which incorporates
     Hebbian and error-driven learning, together with a
     k-Winners-Take-All competitive activation function, into a
     single coherent framework, which is biologically based.  See
     "Computational Explorations in Cognitive Neuroscience:
     Understanding the Mind by Simulating the Brain", by O'Reilly and
     Munakata, MIT Press, 2000 (September) and associated simulations
     *Note intro:: for details.

`lstm++'
     The long-short-term-memory algorithm by Hochreiter, Schmidhuber
     et al.

`rns++'
     The real-time neural simultation program developed by Josh Brown.

`maketa'
     The type-scanner used for programming the software.  You can
     read about it in *Note prog::.

`css'
     A stand-alone version of the CSS script language system.  It can
     be used as an interpreted C++ language system for any number of
     tasks.

   You should either add the path to these binaries to your standard
path, or make symbolic links to these files in `/usr/local/bin' or
some similar place which most user's will have on their path already.
For example, in a csh-like shell (e.g., in the `~/.cshrc' file that
initializes this shell), add (for the LINUX CPU):
     set path = (/usr/local/pdp++/bin/LINUX $path)
   or to make the symbolic links, do:
     cd /usr/local/bin
     ln -s /usr/local/pdp++/bin/LINUX/* .

Configuring the Libraries
=========================

   IMPORTANT: Most of the binaries are dynamically linked, which means
that the `pdp++_version_bin_CPU.tar.gz' file installed some dynamic
libraries in the `PDPDIR/lib/CPU' directory and in the
`PDPDIR/interviews/lib/CPU' directory.  When one of the PDP++
programs is run, it will need to know where to find these dynamic
libraries.  Thus you must set the `LD_LIBRARY_PATH' environmental
variable (using `setenv' under csh/tcsh) to point to both of these
locations.  For example, under LINUX with the standard PDPDIR:
     setenv LD_LIBRARY_PATH /usr/local/pdp++/lib/LINUX:/usr/local/pdp++/interviews/lib/LINUX
   It is a good idea to put this setting in your initialization file
for your shell (i.e. `~/.cshrc').

   It might be easier, especially if you want to use idraw or other
programs available under interviews, to copy the
`PDPDIR/interviews/lib/CPU/libIVhines.so*' (or .sl for HP800) file
into your `/usr/local/lib' or somewhere else that is already on your
dynamic linker's path.

   ADDITIONAL STEPS FOR libIV: First, note that we are now using (as
of version 3.0) the version of InterViews maintained by Michael Hines
as part of the NEURON detailed neural simulation package.  This
library is now called `libIVhines'.  The latest version should always
be available at `ftp://www.neuron.yale.edu/neuron/unix/', with the
current version being `iv-15.tar.gz' (also available on the PDP++ ftp
sites).

   The interviews library in `PDPDIR/interviews/lib/CPU' is called
libIVhines.so.3.0.3 (or possibly other numbers instead of 3.0.3), but
on several unix systems (including LINUX, SGI, SUN4) the linker also
wants to see a libIVhines.so.3 and a libIVhines.so as different names
for this same file.  Therefore, you need to do the following in
whatever directory you end up installing libIVhines.so.3.0.3 (even if
you keep it in the original location, you need to do this extra step):
     ln -s libIVhines.so.3.0.3 libIVhines.so.3
     ln -s libIVhines.so.3.0.3 libIVhines.so
   (replace 3.0.3 and .3 with the appropriate numbers for the
libIVhines file you actually have).  Note that the .rpm install under
LINUX does this automatically.

Manual
======

   The manual is distributed in several versions, including a
postscript file that can be printed out for hard-copy, a set of
"info" files that can be installed in your standard info file
location and added to your `dir' file for reading info files in gnu
emacs and other programs, and a directory called `html' which
contains a large number of `.html' files that can be read with
`Netscape', `Mosaic' or some other WWW program.  Point your program
at `pdp-user_1.html' for the chapter-level summary, or
`pdp-user_toc.html' for the detailed table of contents.

Help Viewer Configuration
-------------------------

   There is now a `Help' menu item on all of the objects (under the
`Object' menu), which automatically pulls up the appropriate section
of the `html' version of the manual, using `netscape' by default.

   Under Windows (CYGWIN), the default help command is setup to use
`C:/Program Files/Internet Explorer/iexplorer.exe' -- if you prefer
netscape or any other HTML browser, this should be changed to the
full path to that executable.

   Under Mac (DARWIN), the default help command is `open -a
\"Internet Explorer\" %s &' - this should work for most systems,
although you might want to use the new browser whatever it is called.

   These defaults can be changed in the `Settings' menu of the
`PDP++Root' object, see *Note proj-defaults:: and *Note gui-settings::
for details.

   The latest version of the manual is also available on-line from:
       http://psych.colorado.edu/~oreilly/PDP++/PDP++.html

MS Windows Configuration
========================

Memory Configuration
--------------------

   The PDP++ executables are compiled using an environment called
cygwin developed by Cygnus Solutions (now owned by Red Hat Software).
The default cygwin configuration has an upper limit of 128MB, which
should work for most simulations.  However, you might want to
increase this limit if you are exploring larger simulations.

   Open regedit (or regedt32) and find the key
HKEY_CURRENT_USER\Software\Cygnus Solutions\Cygwin\

   If this does not exist, you must create a new Key called Cygnus
Solutions, and then another within it called Cygwin.

   Then, create a new DWORD value under this key called
"heap_chunk_in_mb" that contains the maximum amount of memory (in Mb)
your application needs (watch the hex/decimal toggle - you'll
probably want to set it to decimal).  For example if you wanted to set
the memory limit to 256Mb, just enter 256.  Exit and restart all
cygwin applications (e.g., pdp++).

Taskbar Configuration
---------------------

   Because the simulator uses many windows, the windows taskbar often
does not adequetly display the names of the windows.  This can be
remediated by dragging the top of the bar up, allowing more room for
each icon.  Another approach is to grab the taskbar, drag it to the
right edge of the screen, drag the left edge to widen it, and then
set it to auto hide (right click on the taskbar, and select
Properties to expose this option, or go to the Start
menu/Settings/Taskbar).  When all this has been done, the window list
can be exposed bringing the pointer to the right edge of the screen.

Mac OS-X Configuration
======================

   PDP++ depends on having an XWindows (X11) server running on your
mac.  Apple now has their own version of the XFree86 X11 server,
which runs very smoothly under the standard OSX window manager.  This
is the recommended solution.  Read more about it and download from:

   `http://www.apple.com/macosx/x11/'

   Basically, everything works just as under unix because this is a
fairly standard unix setup after all is said and done.  The manual
should therefore provide all the info you need.

Other Configuration
===================

   See *Note proj-defaults:: for instructions on how to setup
customized startup files if you want to change some of the default
properties of the system.

   Happy simulating!


File: pdp-user,  Node: inst-prog,  Prev: inst-users,  Up: inst

Installing the Programmers Version
==================================

   Read over the instructions for installing the end-user's version
first.  This assumes that you have unloaded the
`pdp++_version_src.tar.gz' file in something like `/usr/local/pdp++'.

   IMPORTANT: Whenever you are compiling, you need to have the
environmental variable `CPU' set to reflect your machine type (see
above).  Other machine types can be found in the interviews
`config/InterViews/arch.def' directory.  These are (in addition to
the above): `VAX, MIPSEL, SUN3, SUNi386, SUN, HP300, HP200, HP500,
HP, ATT, APOLLO, SONY68, SONYmips, SONY, PEGASUS, M4330, MACII, CRAY,
STELLAR, IBMi386, IBMrt, IBMr2, LUNA68, LUNA88, MIPSEB, MOTOROLA,
X386, DGUX, CONVEX, stratus, ALPHA'.

   The C++ compiler types that are supported are modern gnu g++/egcs
compilers (anything released after 1999 seems to work), and
proprietary system compilers, which we refer to as  `CC' compilers.

   There are a couple of other libraries that the PDP++ software
depends on.  These need to be made before PDP++ itself can be
compiled.  Please ensure that all of the following are installed
properly:

   1) The `readline' library, which will have already been installed
if `gdb' or perhaps other gnu programs have been installed on your
system (or if you are using Linux).  Look for
`/usr/lib/libreadline.so' or `/usr/local/lib/libreadline.so'.  If it
isn't there, then download a version of it from one of the gnu ftp
server sites (e.g., `gnudist.gnu.org'), and compile and install the
library.

   2) If using g++, and not on a Linux-based system, you need to make
the the `libstdc++' library in the libg++ distribution.  NOTE: PDP++
now requires the `sstream' header file, which defines the
`stringstream' class, which is a much improved replacement for the
`strstream' class.  For g++ version 2.9x, this header file might be
missing or broken (e.g., under RedHat 7.3, it is present but
broken!).  A good version of this file is present in
`PDPDIR/include/new_sstream_for_gcc_2.9x.h', and should be installed
in the appropriate location (e.g., for RH 7.3, do the following as
root: `cp include/new_sstream_for_gcc_2.9x.h
/usr/include/g++-3/sstream'.

   I haven't done this install in a while so the following is likely
out of date:. It seems that in the latest distribution of libg++ both
of these are installed in `/usr/local/lib' automatically, but if they
are not there, `libiostream.so' is made in the `libio' directory in
the libg++ distribution (do a `make install' to get the properly
installed or copy it yourself), and `libstdc++.so' is made in the
`libstdc++.so' directory.  CC/cfront users should have their iostream
code linked in automatically via the standard C++ library that comes
with the compiler.

   3) Install the InterViews library, which provides the graphics
toolkit used by PDP++.  We have collaborated with Michael Hines,
developer of the Neuron simulation system, in developing an improved
version of the InterViews library - You must install the Hines
version of InterViews!.  The source code for this version is:

     iv-15.tar.tz

   available on our ftp servers, and the latest version should be
available from Michael Hines' ftp site at:
`ftp://www.neuron.yale.edu/neuron/unix/'.  Some miscellaneous
information about interviews can be found in the
`PDPDIR/lib/interviews' directory.

   An alternative to compiling interviews yourself is to download
pre-compiled interviews libraries from us.  These are available for
the dominant form of compiler (CC or g++) for the platforms on which
the binary distribution is available (see list above in *Note inst::)
and are provided as `pdp++_version_ivlib_CPU_CC.tar.gz' for the CC
compiler and `..._g++.tar.gz' for g++.  The include files, which are
necessary to use the libraries to compile PDP++, are in
`pdp++_version_ivinc.tar.gz'.  These create a directory called
`interviews' when extracted, which means this should be done in the
`/usr/local' directory so that the interviews directory is
`/usr/local/interviews'.  Alternatively, these can be installed
elsewhere and the `IDIRS_EXTRA' and `LDIRS_EXTRA' makefile variables
set to point to this directory (see below).  We install ours in
`/usr/local/lib' - follow the directions in the End Users's install
version described above to do this.

   4) Once you have the above libraries installed, the next step is to
configure the makefiles for the type of compiler and system you have.
These makefiles are located in `PDPDIR/config'.  The actual makefiles
in a given directory (e.g., `src/bp/Makefile' and
`src/bp/CPU/Makefile') are made by combining several makefile
components.  `config/Makefile.std' has the standard rules for making
various things, `config/Makefile.defs' has the standard definitions
for everything, and then `config/Makefile.CPU' overrides any of these
definitions that need to be set differently for a different CPU type
(i.e., "local" definitions).  There are several sub-steps to this
process as labeled with letters below:

   a) If you have installed the software in a location other than
`/usr/local/pdp++': You need to change the definition of `PDPDIR' in
both the top-level Makefile (`PDPDIR/Makefile') and in the
definitions makefile `PDPDIR/config/Makefile.defs'.

   There are makefiles in `PDPDIR/config' for the supported CPU types
listed above, and the two different supported compilers (g++, CC). The
makefiles are named `Makefile.CPU.cmplr', where cmplr is either `g++'
or `CC'. The actual makefile used during compiling for a given
machine is the one called `Makefile.CPU', where CPU is the type of
system you are compiling on (e.g., LINUX, SUN4, HP800, SGI, etc.).
If you are compiling on a machine for which a standard makefile does
not exist, copy one from a supported machine for the same type of
compiler.  Also, see the notes below about porting to a new type of
machine.

   b) Copy the appropriate `Makefile.CPU.cmplr' makefile (where cmplr
is either g++ or CC depending on which compiler you are using), to
`Makefile.CPU' (again, CPU is your machine type, not 'CPU'). For some
architectures there is just one `Makefile.CPU' since only one type of
compilier is currently supported. In this case you can just leave it
as is.

   c) You should put any specific "local" definitions or
modifications to the makefiles in the `config/Makefile.CPU'.  This
will be included last in the actual makefiles, and any definitions
appearing here will override the standard definitions.  To see the
various definitions that might affect compiling, look at
`Makefile.defs', which contains all the "standard" definitions, along
with descriptive comments.  The following are items that you will
typically have to pay attention to:

   i) `IOS_INCLUDES' and `IOS_LIB': In order to be able to access via
the CSS script language the functions associated with the standard
C++ iostream classes, the type-scanning program `maketa' needs to
process the iostream header files: `streambuf.h', `iostream.h',
`fstream.h', and `strstream.h'.  These files are scanned in the
`src/ta' directory, as part of the building of the type access
library `libtypea.a'.  These header files are different depending on
the compiler being used.  For CC compilers, the `IOS_INCLUDES'
variable should be set to `CC-3.1'.

   These header files are typically located in `/usr/include/CC',
which is where the `CC-3.1' versions of these files in `src/ta' point
to via symbolic links.  Thus, if your headers are located elsewhere,
you will need to change these symbolic links, or just copy the header
files directly into the `ios-CC-3.1' subdirectory in `src/ta'.  For
g++ users, `IOS_INCLUDES' variable should be set to `g++-2.8.1' (for
g++ 2.9x) or `g++-3.1' (for g++ 3.x).  As of version 3.0, these g++
iostream headers are never actually included in the compile process
itself, and are only scanned via the maketa program.  Therefore, they
have been dramatically edited to expose only the relevant interface
components.  As such g++-3.1 should work for all subsequent releases
of g++ (hopefully!).

   Note that you can use the `make force_ta' action to force a re-scan
of the header files.  A `make opt_lib' is then neccessary to compile
this type information into the library.  Finally, the `IOS_LIB'
variable should be blank by default for both g++ and C++ users (for
newer g++), but for older g++ configurations it was necessary to set
it to `-liostream'.

   ii) `IDIRS_EXTRA' and `LDIRS_EXTRA' can be used to specify
locations for other include and library files, respectively (for
example, the cfront compiler may need to be told to look in
-I/usr/include/CC for include files).  Use these if you have installed
any libraries (e.g., InterViews) in a non-standard location.

   iii) `MAKETA_FLAGS' should be set to `-hx -css -instances' by
default, but it is also often necessary to include the include path
for the iostream and other C++ library files.  For example, the LINUX
makefile has the following: `LIBG++_INCLUDE_DIR =
-I/usr/include/linux -I/usr/include/g++-3', and then `MAKETA_FLAGS =
-hx -css -instances $(LIBG++_INCLUDE_DIR)'.

   iv) `cppC': this is the c-pre-processor, needed for the `maketa'
program to process header files.  Although the system default
preprocessor, usually installed in `/usr/lib/cpp', should work,
`maketa' was developed around the gnu cpp program, and so if you run
into difficulties using the system cpp, install the gnu one (included
as part of the gcc compiler).  Note that `make install' of gcc/g++
does not apparently install this program by default, so you have to
manually copy it from either your gcc compile directory or
`/usr/lib/gcc-lib/<machine>/cpp'.  You often need to include a define
for the system architecture (e.g., `-D__i386__' for Linux on intel
chips, or `-Dsparc' for suns) in the cppC command.

   v) `VT_xxx' and `TI_xxx': these specify the virtual-table
instantiation and template-instantiation (respectively) files, which
are needed by different compilers.  CC typically requires the VT_xxx
files along with the +e[01] flags to only make one copy of virtual
tables, while g++ requires the TI_xxx files to only make one copy of
the templates.  The TI_xxx files are included by default, so you will
need to define them to be empty to override this default:
`TI_INST_SRC = ', `TI_INST_OBJ = ', `TI_INST_DEP = '.

   vi) Porting to a non-supported machine: There are a small set of
system-dependent definitions contained in `src/ta/ta_stdef.h', which
are triggered by defines set up in the makefiles.  `NO_BUILTIN_BOOL'
should be defined if the c++ compiler does not have a bulitin `bool'
type, which is the case with most cfront-based CC compilers, but not
g++.  `CONST_48_ARGS' determines if the `seed48' and `lcong48'
functions take const arguments or not.  In addition, different
platforms may require different defines than those that are flagged
in `ta_stdef.h'.  In this case, you will have to edit `ta_stdef.h'
directly.  Please send any such additions, and the corresponding
`config/Makefile.CPU' along with any notes to us
(`pdpadmin@crab.psy.cmu.edu') so we can put them on our web page for
others to use, and incorporate them into subsequent releases.

   vii) For some more information about the makefiles, see *Note
prog-make::.

   5) The standard makefiles use gnu's `bison' instead of `yacc' for
making parsers.  If you don't touch any of the .y files in the
distribution, you won't need either.  If you plan on messing around
with the guts of the maketa type scanner or CSS, then you will
probably want to install the latest version of `bison'.

   6) The dependency information, which is essential if you are going
to be editing the main body of PDP++ code, but not necessary for a
one-pass make of the system, is not made by the default `make world'
action.  If you want to make this dependency information, do it with
a `make depend' after a successful `make world'.  Also, note that the
automatic dependencies are made by calling `gcc' in the standard
configuration.  If your local C compiler supports the `-M' flag for
generating dependency information, then this can be used instead.
Just change the definition for `CC' in your `Makefile.CPU'.  If you
don't have gcc and your local C compiler doesn't support this, you can
edit the end of the `Makefile.std' and change it to use the
`makedepend' program, which we have not found to work as well, but it
is an option.

   7) On some systems, the standard `make' program is broken and will
not work with our complex makfile system.  This is true of the SUN4
system and IBMaix, and may be true of others.  In this case, you will
have to install the GNU make program, and use it to compile the
software.  If you get inexplicable errors about not being able to make
certain things (seems to be the .d dependency files in particular that
cause a problem), then try using GNU make (again, available at
`gnudist.gnu.org' or mirrors).

   8) If your CPU supports shared libraries (most do now), you will
need to insure that the `LD_LIBRARY_PATH' environment variable
includes the path `PDPDIR/lib/CPU' nad `PDPDIR/interviews/LIB/CPU'
where PDPDIR is the location of the pdp++ distribution, and CPU is
your system type as described above.  This is important even during
the compile process, because the maketa program will need to access
shared libs.

   After setting appropriate definitions, go back up to the PDPDIR
and just do a:
     make world

   this should compile everything.  This will make makefiles in each
directory based on your CPU type, and then compile the various
libraries and then the executables.

   Most likely, the make will at least proceed past all the basic
directory initialization stuff that is part of `make world'.  Thus,
if the compile stops after making the makefiles and after making the
`maketa' program, you can fix the problem and re-start it by doing
`make all' instead of `make world'.

   If you run into difficulties during the compile process, the
programming guide might contain some useful information for debugging
what is going wrong: *Note prog::.


File: pdp-user,  Node: over,  Next: tut,  Prev: inst,  Up: Top

Conceptual Overview
*******************

   This chapter provides a general introduction to the PDP++ software
and conceptual overview of its major components.  The primary
objective here is to explain the important design decisions that went
into the software, so that you will be able to get the most out of
it.  This is oriented towards those who have some familiarity with
neural networks, and who want to find out how things are done in this
software.  However, it is also useful for all users to be aware of
the rationale for the way things are set up, so that it becomes
clearer where to look for ways to solve problems that might arise, or
accomplish various simulation objectives.

* Menu:

* over-objs::                   Overview of Object-Oriented Software Design
* over-stru::                   Overview of the Main Object Types
* over-spec::                   Separation of State From Specification Variables
* over-scripts::                Scripts: General Extensibility
* over-group::                  Groups: A General Mechanism for Defining Substructure


File: pdp-user,  Node: over-objs,  Next: over-stru,  Prev: over,  Up: over

Overview of Object-Oriented Software Design
===========================================

   The PDP++ software is based on the principles of object-oriented
software design.  The effects of this are pervasive, and an
understanding of these basic principles can help the user to
understand the software better.

   First of all, everything in the software is an object: units,
connections, layers, networks, etc.  While this sounds intuitive for
these kinds of objects, other kinds of objects, like the object that
controls the way a network is viewed, or the ones that control the way
the network is trained, are somewhat less intuitive.  However, if you
just think of an object as representing some discrete piece or aspect
of the software, it makes more sense.  Essentially the PDP++ software
is just a big collection of objects that work together to let you get
things done.

   The primary advantage of an object-oriented design is its
_flexibility_.  Because the functionality is broken up into
object-sized pieces, these pieces can be combined in many different
ways to accomplish many different tasks.  However, this power comes
at the price of _simplicity_--since everything is accomplished by the
interactions between multiple objects, setting things up to perform
any given task requires knowing which objects to use and how to
configure them.  The very distributed nature of the processing makes
it less easy to identify one single thing that needs to be operated
on in order to do something.  Its like the difference between a
neural network and a conventional computer program, in some sense.
In a computer program, things proceed in a relatively linear,
straightforward fashion from one step to the next, whereas in a
neural network, processing is distributed among many different units
that all work together to get something done.

   While the tradeoff between flexibility and simplicity is
inevitable to a certain extent, we have endeavored to make the
commonly-used tasks simpler by including menus or buttons that do all
of the right things in one simple step.  However, as you become a
more advanced user and want to perform tasks that have not been
"pre-compiled" in this way, expect to encounter some of the
difficulties associated with object-oriented software.


File: pdp-user,  Node: over-stru,  Next: over-spec,  Prev: over-objs,  Up: over

Overview of the Main Object Types
=================================

   This section describes the main ways in which the many tasks
involved in neural network simulation have been divided up and
assigned to different types of objects.  It is essential to
understand this division of labor in order to use the software
effectively.

   There are four central components to a neural network simulation
as we see it:
   * The Network, including layers, units, connections, etc.  (*note
     net::).

   * The Environment on which the network is to be trained and tested
     (*note env::).

   * The Processing or Scheduling of training and testing a network,
     which determines how long to train the network, with what
     environment, etc. (*note proc::).

   * The Logging of the results of training and testing.  One can
     view these results as a graph, a table of numbers, or a grid of
     color chips that represent values in a graphical similar to that
     used in the viewing of networks (*note log::).

   Since all of these components work together in a given simulation,
we have grouped them all under a Project object (*note proj::), which
also has some additional objects (defaults and scripts) that make life
easier.  Thus, when you start a simulation, the first thing you do is
create a new project, and then start creating networks, environments,
etc. within this project.

   New for 2.0: There is now a graphical interface for the
project-level organization of objects, which can really help make
complex projects more managable, especially for dealing with the
processing aspects of things.  Just enlarge your project window on
existing projects to see the new view.

   New for 3.0: The complexity of dealing with distributed objects is
partially simplified through the introduction of a Wizard object that
automates the contstruction of typically-used network simulation
objects.  See *Note how-wizard:: for details.  Also new are principal
components analysis (PCA) and multidimensional scaling (MDS)
techniques for analyzing network representations (*note
env-analyze::), and much imporoved graph log features for displaying
overlapping traces of data, including a spike raster plot.  In
addition, distributed memory parallel processing support, both for
connection-level and event-level parallelism, was added.

   Of the different simulation components, Processing (aka
Scheduling) is probably the least intuitive.  Indeed, we might have
decided instead that networks knew how to train themselves, or that
environments would know how to train networks.  The main reasons we
chose to make Processing a separate major category of function in the
software are: 1) The ways in which one trains and tests a network are
common to different types of networks and learning algorithms. 2)
Many different kinds of training and testing processing can be
performed on the same network and environment.  3) Processing depends
in part on the nature of the network and the nature of the
environment, so that putting this task in either of these separately
would lead to strange dependencies of networks on environments or
vice versa.

   Thus, one can think of the processing function as being similar to
that of a movie director, who coordinates the actors and the camera
operators, set designers, etc. to produce a finished product.
Similarly, the processing coordinates the network with the environment
and other elements to direct the overall process of training and
testing.  For more information on how processing is implemented in the
software see *Note proc-sched::.

* Menu:

* over-stru-net::               The Objects in Networks
* over-stru-env::               The Objects in Environments
* over-stru-proc::              The Objects Involved in Scheduling


File: pdp-user,  Node: over-stru-net,  Next: over-stru-env,  Prev: over-stru,  Up: over-stru

The Objects in Networks
-----------------------

   For the most part, the objects that make up a network are fairly
intuitive, consisting of Layers (*note net-layer::), Units (*note
net-unit::), and Connections (*note net-con::).  However, there is an
additional type of object which describes the pattern of connectivity
between all of the units in one layer to those in another layer,
which is called a Projection (*note net-prjn::).  The Projection is
very useful because it allows one to specify connectivity at a broad
level, instead of individually connecting each unit with other units.
It also seems to be a conceptually real entity--one often thinks
about the connectivity in terms like, "the hidden layer is fully
connected to the input layer."

   The use of Projection objects is illustrative of a general
principle regarding network structure, which is that the larger-scale
objects (Layers and Projections) contain _parameters_ for how to
construct the smaller-scale objects (Units and Connections).  This
allows one to create Layers and Projections, fill in some parameters
regarding the type of connectivity and number and type of units, and
execute general-purpose `Build' and `Connect' (*note net-net::)
functions which translate the parameters into an actual network with
interconnected units.

   While it is possible to manually build a network unit-by-unit and
connection-by-connection, it is usually much easier to use the
parameters-and-build/connect method.  However, some people find the
distinction between specifying and actually building difficult to work
with, in part because only after you execute `Build' and `Connect'
does your actual network reflect the parameters you have entered.  We
have tried to lessen these difficulties by providing graphical
representations of the parameters, so that you can see what you are
specifying at the time you do it, not only after you have built the
network.


File: pdp-user,  Node: over-stru-env,  Next: over-stru-proc,  Prev: over-stru-net,  Up: over-stru

The Objects in Environments
---------------------------

   We decided to call the training and testing data that is presented
to a network an `Environment' (*note env::) to capture the idea that a
network is like an organism that exists and interacts within a
particular environment.  Thus, the environment object describes a
little world for the network to roam around in.  It does not,
however, specify the path that the network takes in its travels
(i.e., the order in which the events are presented to the
network)--this is determined by the scheduling objects, which can be
thought of as "tour guides" in this context.  To switch metaphors, an
environment should be thought of as a library, which contains lots of
data, but does not tell you which books to check out when.

   The `Environment' is composed of `Event's (*note env-event::),
which represent a collection of stimuli or `Pattern's that go
together.  Thus, in the context of a typical backpropagation network,
an `Event' consists of an input pattern and a target pattern for a
single training example.  `Pattern's contain an array of values which
get mapped onto the units in a particular layer of the network.

   In order to represent _sequences_ of events that should be
presented sequentially in time, events can be grouped together (*note
env-seq::, *note over-group::).  Note, however, that the scheduling
objects need to be made aware of the fact that the events in a group
represent a sequence of events in order for these to actually be
presented to the network in the right order (*note
proc-special-seq::).

   Environments can also be defined which dynamically create events
on-line, possibly in response to outputs from the network.  This must
be done under the supervision of an appropriate Schedule process,
which can coordinate network outputs with the parameters that
determine how the environment creates events.

   New for 2.0: The Environment interface has been completely
rewritten, and now enables interactive configuration of event
structures, just like in the Network interface.

