This is pdp-user, produced by makeinfo version 4.1 from pdp-user.texi.


File: pdp-user,  Node: tut-using-monitoring,  Next: tut-using-changing,  Prev: tut-using-running,  Up: tut-using

Monitoring Network Variables
----------------------------

   It is often useful to monitor the values of network variables,
such as the activation of the output units, in your log files.  For
example, you may wish to watch how the network gradually
differentiates its output in response to the four input patterns.  We
can do this by creating what is called a MonitorStat statistic.

   Conceptually, a monitor statistic is a special type of statistic
that simply monitors values already computed by the processes that are
applied to the network.  For example, activations are calculated by
the TrialProcess during training.  To record these activations in a
log we need to monitor them and that's what a Monitor Statistic will
do.  Think of it as an electrode for recording data at specified
times inside your network.

   To create a Monitor statistic, first select the units and the
variable you want to monitor in the NetView.  To monitor with
activation of the output unit, for example, we would make sure `act'
is selected as the variable, and then we would select the output
unit.  In this case we can select either the unit or the whole layer,
it won't make any difference.  To select the layer, make sure the
Select button is highlighted in the NetView and then click over the
output layer itself.  The layer's border will be highlighted.  Now
Select Monitor Values / New (located in the middle-left region of the
netview), and a pop-up will appear with some necessary information
displayed.

   The popup indicates that you are creating a type of Statistic
called a MonitorStat, and it indicates which process will contain this
Statistic.  In this case, the `In Process' button will indicate that
the statistic will be placed at the end of the "TrainTrial" process,
which is what we want (we already have something monitoring
activations in the "TestTrial" process).  The `For Variable' should
be `act'.  The `Using Operator' in this case is `COPY', which is also
what we want - we just want to copy the values of all the selected
units into the statistic.  The last aspect of this is conceptually
the most challenging.  What we need to do is to specify that we want
to `Create Aggregates' of this variable at higher levels.  In this
case, what we really want to do is just copy the activation from all
4 patterns to the Epoch level, so we can display the activations of
the output unit for the different events in our Graph.  So click the
CreateAggregates toggle, then click Ok. Another popup will then
appear, asking you what form of aggregation you want.  In this case
you must select COPY, to indicate that we are copying each event's
output unit activation into the epoch graph, not averaging or summing
or anything like that.  Do this, click Ok, and you are done.

   Before you restart the simulation, you'll need to reinitialize the
weights.  We do this in the Train Control Panel.  You reinitialize
with the same starting weights by clicking Re Init, or you can
initialize with new starting weights by clicking New Init.  Either
way, the network is reinitialized and the epoch counter is set to 0.
Click Re Init this time so we can see trace the learning process over
the very same trajectory as before.  To run the simulation again we
can simply press Run or Step in the "Train" process control panel.
You'll see the new results graphed on the same graph along with the
earlier sum of sum-squared-error results.

   The graph may seem a little confusing at first.  Overlaying the
`sum_sum_se' results will be four new lines, which will seem to be
oscillating rather extremely.  Buttons identifying these new lines
will be shown on the left of the View.  Note that the color of the
graph line associated with a variable that is displayed is shown to
the left of the Button, and the color of the axis that is associated
with the variable is displayed on the right.  All four new lines are
associated with a new Y-axis, which you can see covers a rather
narrow range of values (about .46 to .54).  This axis is tied to the
entire group of activations that are being monitored, and it is
auto-scaled to the largest and smallest value in the display.

   You may want to fix the scale to something reasonable like 0-1.
To do this, click with the _right_ mouse button -- the Edit button -
on the lead (top) element of the set of new elements (it may be
labeled `output.act' or `cpy_output.act_0'.  A popup will appear.
You want to set `min mode' and `max mode' to `FIXED' so click where
it says `AUTO GROUP' and select `FIXED', then enter 1.0 in the max
field next to `range'.  To make this take effect, click Update View,
then click Ok.  The new Y axis will now span the 0-1 range, and the
oscillations will appear more moderate on this scale.  This axis
adjustment can be done while the simulation is running.

   In any case, you'll see that after thirty epochs or so the
oscillations level off.  The simulation appears to be flatlined until
about epoch 120, when the activations of the output unit begin to
differentiate the four input patterns. (The outputs to patterns 1 and
2, which are the 01 and 10 patterns, are nearly identical and the
latter lies on top of the former much of the time).  In any case, you
should see that the network tends to produce the strongest output for
the '11' pattern (pattern 3) until about epoch 210, well after it has
completely nailed the '00' case.  After epoch 210 the performance
changes rapidly for the 10, 01, and 11 patterns (patterns 1, 2, and
3) until about epoch 270 or so, where they begin to level off at
their correct values.


File: pdp-user,  Node: tut-using-changing,  Next: tut-using-saving,  Prev: tut-using-monitoring,  Up: tut-using

Changing Things
---------------

   Our network has succeeded in learning XOR, but none too quickly.
The question arises, can backpropagation do a better job?  The first
thing that might occur to you to try is increasing the learning rate,
so let's try that.

   Before you can change the learning rate, you must understand how
such variables are handled in the simulator.  The simulator is set up
so that it is possible to have different learning rates for different
weights.  This would, for example, allow us to have one learning rate
for the weights from the input layer to the hidden layer, and a
different learning rate for the weights from the hidden layer to the
output layer.  To allow this, parameters such as the learning rate
are stored in structures called _specifications_ or Specs.  Each bias
weight and each group of receiving weights (i.e., all the weights
coming to a particular unit from a particular layer) has associated
with it a pointer to a specification, and parameters such as the
learning rate are read from the specification when they are used.
Our network has been set up so that all of the weights and biases
share the same specification.  This specification is called a
connection specification or ConSpec, and since it is the one we are
using in this XOR network we have called it "XORConSpec".

   One can look at this specification by selecting in the Project
window .specs / Edit / XORConSpec.  When this pops up we can see that
there are actually several variables associated with this Spec,
including `lrate', `momentum', and `decay'.  We can change any one of
these, but for now click in the `lrate' field and change the value
to, say, 1.0.  Then click Apply or Ok (which applies and closes the
pop-up).

   To see what happens with this higher learning rate, go ahead and Re
Init on the Train Control panel as before.  Before you hit Run you
may wish to Clear the GraphLogView.

   One can modify the learning process in other ways. For example,
one can change the `momentum' parameter, or introduce `decay'.  These
parameters are also associated with the ConSpec and can be modified
in the same way as the `lrate'.  For more information about these
parameters, see *Note bp-con::.

   Another parameter of the Backpropagation model is the range of
values allowed when initializing the connection weights.  This is
controlled by the `var' parameter of the `rnd' field of the ConSpec.
The default value of 0.5 can be increased to larger values, such as
1.0, thereby giving a larger range of variation in the initial values
of the weights (and biases).

   One can also modify the learning process by selecting different
options in the EpochProcess.  For example, as initially configured,
the XOR problem is run using `SEQUENTIAL' order of pattern
presentation, and the mode of weight updating is `BATCH'.
`SEQUENTIAL' order simply means that the patterns are presented in
the order listed in the environment.  The other two options are
`PERMUTED' (each pattern presented once per epoch, but the order is
randomly rearranged) and `RANDOM' (on each trial one of the patterns
is selected at random from the set of patterns, so that patterns only
occur once per epoch on the average).

   The options for weight updating are `BATCH', which means that the
weights are updated at the end of the epoch; `ON LINE', which means
that the weights are updated after the presentation of each pattern,
`SMALL BATCH', which means that the weights are updated after every
`batch_n' patterns, and `TEST', which means that the weights are not
updated at all.

   These characteristics of the processing are managed by the
"Epoch_0" process that is subordinate to the "Train_0" process-- and
so to change them select .processes / Edit / Epoch_0 from the Project
window menu bar.  The dialog that pops up indicates the current
values of these options (sequential and batch) next to their labels
(`order' and `wt update').  Click on the current value of the option
and the alternatives will be displayed; then select the one you want,
and click Apply or Ok when done.


File: pdp-user,  Node: tut-using-saving,  Prev: tut-using-changing,  Up: tut-using

Saving, restoring, and exiting.
-------------------------------

   You can save the state of your simulation or virtually any part of
it.  This is done by selecting Object / Save or Object / Save As on
the appropriate menu.  For example, to save the whole project using
its existing project name, you would select Object / Save from the
Project window.  To save just the network in the file "myxor.net" you
would select Object / Save As from the NetView window and then enter
the filename "myxor" in the filename field of the popup before
clicking Ok.  The program will actually save the file as
"myxor.net.gz"; the ".gz" suffix indicates that the file has been
compressed using `gzip'.

   To restore the state of the simulation or some part of it, you can
just select Object / Load in the appropriate window.  This will load
the saved copy of the object over whatever is presently there,
effectively destroying whatever was there before.  When you first
start up the simulator, you can load a project by selecting .project /
Open In / Root.  This creates a project as a constituent of root and
loads the project saved in the file.

   At the end of the day, once you've done whatever saving you want
to do, you can exit from the simulator by selecting Object / Quit from
the Root window.

   Now you know how to run the software and you are done with this
part of the tutorial.  The next section of the tutorial steps through
the process of actually building a simulation from scratch instead of
loading a "canned" one in.


File: pdp-user,  Node: tut-config,  Prev: tut-using,  Up: tut

Configuring the Encoder Problem
===============================

   Running a canned exercise may be fun and informative, but most
people like to use simulation models to work on something new of
their own.  In this short section we will give you a quick overview
of the essentials of creating your own project.  We'll use the 4-2-4
encoder problem, first considered by Ackley, Hinton and Sejnowski in
their seminal article on learning in Boltzmann machines.  We'll stay
with the back propagation learning algorithm, though, since it learns
the problem much faster than the Boltzmann machine.

   In the 4-2-4 encoder problem, the goal is to learn to associate an
input pattern with itself via a small hidden layer (in this case
containing 2 units).  There are 4 input units and 4 output units, and
there are 4 training pattern-pairs, each involving one unit on, the
same unit, in both the input and the output:

     Input   Output
     1000 -> 1000
     0100 -> 0100
     0010 -> 0010
     0001 -> 0001

   What we need to do is set up a network with four input units, four
output units, and two hidden units; then create an environment
containing the set of training patterns; then create the processes
that train and test the network, then create Logs and LogViews for
displaying the results of training and testing.

   To begin, you just start up the executable for bp by typing `bp++'
at your unix prompt.  This starts the program, giving you the `bp++'
prompt as your command-line interface to the program, and giving you
a Root window as the seed of the GUI you are about to build for
yourself.  You're going to do the whole thing through the GUI.  You
can keep a script of what we've done so you can repeat the same steps
(or even edit them and do something slightly different) later.  This
is optional, however; many people prefer simply to save the state of
their configured project in a .proj file.

   To create your project, select .projects / New / Project in the
Root window, and then just click Ok on the confirmation box that
appears. A project menu will appear.

* Menu:

* tut-config-scripts::          Recording a Script File of Your Actions
* tut-config-networks::         Making a New Network
* tut-config-environments::     Making a New Environment
* tut-config-processes::        Setting Up Control Processes
* tut-config-logs::             Creating Logs For Viewing Data
* tut-config-misc::             A Few Miscellaneous Things
* tut-config-running::          Training and Testing Your Network


File: pdp-user,  Node: tut-config-scripts,  Next: tut-config-networks,  Prev: tut-config,  Up: tut-config

Recording a Script File of Your Actions
---------------------------------------

   If you would like to make a script file of your actions, you should
start it up before you start building the network.  The script will
record all your actions, which means you can later look at this script
and see what steps you took (and replay them).  However, because one
inevitably makes mistakes along the way, using a script to save
everything is not the best idea -- instead, we simply save a project
file at the end with everything stored as it is currently configured.

   To start recording your script, select .scripts / New / Script ,
and Ok the creation with the _right_ mouse button (this tells the
program you want to edit the object that you create).  An edit dialog
window for the script object should automatically appear. If you
forgot to use the right mouse button, you can manually bring up an
edit dialog by choosing .scripts / Edit / Script_0. You will want to
associate a file with this script, so click on the menu next to
`script file' (where it says `--- No File ---') and select Open. A
File Selection Window will appear with an empty edit field near at
the top under the text "Enter Filename:".  Fill in the name you want
to give your script ("my424" would do) and press the "Open" button at
the bottom of the File Selection Window or press <Enter>.  Once this
is done you still have to click Apply back on the script dialog to
confirm your file name.  Now hit Record and your future actions will
be recorded in the script.  The mouse pointer should change to an
arrrow with the letters "REC" beneath it to indicate that you are in
record mode. To stop recording you can press the StopRec button in
this window. Since we do not need this window until later you can
iconify it now.


File: pdp-user,  Node: tut-config-networks,  Next: tut-config-environments,  Prev: tut-config-scripts,  Up: tut-config

Making a New Network
--------------------

   Now, lets make the Network.  Select .networks / New / Network and
Ok.  You now have a blank Network object, and a NetView window in
which to view it.

   Click New Layer(s), set the `Number' to 3 (by editing the number
field or using the increment/decrement buttons), and hit Ok.  Your
three layers will appear, but at this point none of them contain any
units.  The NetView is now in ReShape mode (the corresponding mode
button is highlighted).  So, if you select "Layer_0", you can start
to reshape it.  Click and hold in the line you see above the layer
label (this line is a layer-frame with no space in it yet) and drag
the mouse to the right; boxes will appear corresponding to
unit-spaces.  When you have 4, stop and let go.  You've created a
frame for four units.  Click on the line above "Layer_1" to create a
frame for the two hidden units; drag right until two boxes appear.
Finally click on the line above the "Layer_2" label, and create a
frame for the four output units. You now have frames but no units; to
create the units, in all three layers at once, just click on Build
All.  The units should all now be visible, and the default act
variable selected, indicating that your are viewing their activation
states, which should all be zero.

   You can move the hidden layer to be centered, by selecting Move
mode then dragging the hidden layer with the mouse.  Note that things
move in "chunks" of the size of a single unit, so you will have to
move the mouse a certain amount before the layer itself moves.  It's
a good idea to go back to Select mode after moving by hitting Select.

   Also, clicking the Init button will adjust the display of the
network to fill the available space, and generally fix any display
problems.  It is automatically performed most of the time, but you can
use the Init button if the display doesn't look right for some reason.

   Now we need to make connections.  Actually, you first make
Projections then create connections within the projections.  A
projection is to a connection what a layer is to a unit; it
represents a bunch of connections that connect units in two layers.
In any case, you need to create two projections.  One to the hidden
layer from the input layer, and one to the output layer from the
hidden layer.  In each case:

   * Select the receiving layer with the left mouse button.  You want
     to see the frame around all of the units highlighted.  This
     should occur on the first click; if that's not what you see,
     keep clicking until it is; (you're cycling the selection through
     the layer, all the units, and the unit under the mouse).

   * Add the sending layer to the selection using the
     "extend-select", which can be done with either the middle mouse
     button, or the Shift key plus the left mouse button, depending
     on whether you come from a Unix or a Mac background.  Now both
     the sending and receiving layers should be highlighted. If you
     make a mistake, click the left mouse button in the background
     area of the NetView between the layers to unselect the layers
     and start over with the previous step.

   * When you have 2 layers selected, click New Prjn(s), which will be
     highlighted. A Projection arrow should appear between the two
     layers.  Note that the arrow head of the projection arrow is
     unfilled (outline) indicating the the actual connections have
     not been created yet.

   After you do this for both projections you will have two unfilled
arrows.  Now click Connect All and your projections will be filled
with connections to each unit on the receiving end from each unit on
the sending end. Now the arrow heads of the projections will be solid
indicating that their connections have indeed been created. You may
verify this by clicking on the r.wt button and using the finger
pointer to view the weights of some of the hidden and output units.
When you are finished looking around, return to viewing the
activations by pressing act.

   And your network is complete.


File: pdp-user,  Node: tut-config-environments,  Next: tut-config-processes,  Prev: tut-config-networks,  Up: tut-config

Making a New Environment
------------------------

   Now we can create the Environment and the training patterns.
Locate the Project window (which you'll note has been updated to
reflect the existence of your new Network object), and select
.environments / New / Environment and Ok.  Your EnviroView window will
appear.  Then click New Event.  Set the number to create to 4, then
click Ok.  You should see Panel buttons for all four events
("Event_0".."Event_3").  Left-click the first one to select it for
viewing, then extend-select (middle-click or Shift + left-click) the
rest in order, and all of them will appear to the right of the
buttons.  They automatically have an input pattern the same size as
the input layer and an output or target pattern the same size as the
output layer, and all the values of all of the patterns are
initialized to 0.  Both input and output patterns are displayed with
the top four boxes of each event represent the output target values
and the lower four boxes representing the input values. You can now
set the 1 bits of each pattern by clicking on the color spot next to
the value field labeled '1', then clicking on the appropriate
elements of the pattern.  They will turn to the appropriate color,
indicating the specified value.  You have to click Apply for the
change to take effect.  Now you have your environment.

   You can also use this EnviroView window to configure the layout of
the patterns within the events according to the EventSpec and
PatternSpec specs.  The default layout (which works for our present
purposes) is to have one pattern for the first layer in the network,
which serves as an input, and another pattern for the last layer in
the network, which serves as an output.  However, these defaults will
not always be appropriate.  To see how to change them, hit the Edit
Specs button at the top left of the window, and then after the
display changes, hit the EventSpec_0 button.  You will see two grids
for the two patterns (input and output).  You can move, reshape, and
edit these patterns if you need to.  The text within each pattern
shows some of the critical settings in terms of whether an pattern is
an `INPUT' or `TARGET', what layer it goes to, etc.  We'll just leave
everything as-is for now, but if for example you wanted to change the
network to be a 8-3-8 encoder, then you'd need to reshape these
patterns to be 8 units wide instead of 4.  Doing so would
automatically stretch the corresponding events that use this spec.

   For now, just hit Edit Events to return to the events display, and
iconify the window.


File: pdp-user,  Node: tut-config-processes,  Next: tut-config-logs,  Prev: tut-config-environments,  Up: tut-config

Setting Up Control Processes
----------------------------

   While the Network and the Environment constitute the bulk of the
custom aspects of a simulation, the way in which the network is
trained can vary depending on what kinds of tasks you are performing,
etc.  The training and testing of a network is determined by the
processes, and the results of running these processes can be viewed
in logs that record data over time.

   Processes come in hierarchies, but there are sensible defaults
built into the program that create sensible hierarchies. We will
create two hierarchies, one for training the network, and one for
testing.  Should your process hierarchy become accidentally
misaligned with unnecessary Train, Epoch and Testing processes or
spurious Batch or Sequence processes, you may wish to start the
process creation procedure over from scratch by choosing .processes /
Remove / All.  In this case, the default naming of the processes may
use a different numbering convention than the one described below,
but hopefully you will be able to follow along.

   To create the TrainProcess, select .processes / New / TrainProcess
from the Project window.  A New Object Popup window will appear
indicating that 1 Train Process should be created. In addition, the
Create Subprocs toggle will be checked; leave this, it will do the
right thing and create a subordinate Epoch and Trial process under
the Train process for you automatically.  Use the right mouse button
to click Ok so you can edit the Train process.  If you forget to use
the right mouse button, you can edit the train process by selecting
.processes / Edit / Train_0. You could at this point rename the train
process to something more descriptive, (e.g., "Training") by changing
its name field, but we'll assume you stay with the default, which is
just Train_N, where N is the index of this instance of a train
process; if this is the first process you are creating, the process
will be called Train_0.  One thing you can notice here is the max
epoch value associated with the Train process.  By default it has
value 1000, which is reasonable for many relatively small scale
problems -- if it doesn't learn in 1000 epochs, it may not learn at
all.  We are done with editing the Train Process at this point, so
you should press Ok and dismiss the Edit Dialog.

   Now, take a look at your Project window, which shows all of the
major objects within your project.  You should see the network and
environment, and the three processes in the process hierarchy you just
created.  You can automatically view or iconify the network or
environment windows by double-clicking on their icons.  Also, you can
see how everything is connected up through the processes by clicking
on a process and hitting show links -- select the yellow Trial_0 icon
and then hit the Show Links button.  You should see that this process
works on the network (solid pink line) and the environment (solid
green line), and that it updates the network display (dashed pink
line).  You can also see the statistics that are computed at each
level of processing (more on this below).

   We can now edit the sub-process Epoch_0, either clicking with the
right mouse button on the Epoch_0 object in the project view, or by
choosing its name from the .processes / Edit / name menu. Again, you
could rename them at this point; but we'll assume you stay with the
default name of Epoch_0.  The main reason to edit the Epoch process
is to specify the presentation order and what mode of weight updating
you want by setting the values of the order and wt update fields in
the edit dialog for Epoch_0.  The defaults, `PERMUTED' and `ONLINE'
may not be what you want (`SEQUENTIAL' `BATCH' is the "standard"
combination).

   By default, the TrialProcess for the Bp algorithm creates a
squared-error statistic.  You can see this statistic in the project
view, or by pulling down the edit menu, and moving the mouse over any
of the processes -- it shows up in a sub-menu off of the these
processes.  In general, statistics are created at the TrialProcess
level, because that is the appropriate time-grain for the computation
of most statistics.  For the squared-error, this is when an
individual pattern has been presented, the activations propagated
through the network, and a difference between the target and actual
output values is available for computing the error.  However, one
often wants to see an aggregation of the error (and other statistic)
over higher levels of processing (e.g., a sum of the squared-error
over an entire epoch of training).  This is done by creating
aggregates of the statistic, and it is why all of the processes
contain a squared-error statistic in them - the ones in the Epoch and
Train processes are aggregations of the one computed in the
TrialProcess.

   While you could procede to the next step now, we will make a small
detour in order to show you how to create statistics, since you may in
the future want to create a different type of statistic than the
default squared-error statistic.  We will simply remove the existing
statistic, and then perform the steps necessary to recreate it.  To
remove the statistic, you can click on it (under the Trial_0 process)
and hit Rmv Obj(s) button in the project view, or select .processes /
Remove / Trial_0 / sum_Trial_0_SE_Stat, and confirm that it is Ok to
close (remove) this item.  Note that the SE_Stat has been removed
from all of the processes -- a statistic will always remove its
aggregators in higher-level processes when it is removed.

   Now we will perform the steps necessary to re-create the statistic
we just removed.  You can do this by either selecting the Trial_0
object in the project view, and hitting New Stat, or by selecting
.processes / New Stat / SE_Stat in the Project window (New Stat is
near the bottom of the menu).  The popup that appears indicates the
type and number; it also indicates which process the stat will be
created in.  As mentioned above, statistics are generally created at
the TrialProcess level, because that is the appropriate time-grain
for the computation of most statistics.  Given that there is only one
Trial process (Trial_0) at this point, the program will guess
correctly and suggest to create the SE_Stat at this level.  The popup
also allows you to choose where within the Trial process to create the
statistic - with the two options being Loop and Final.  Most
statistics will know where to create themselves, so the DEFAULT
selection should be used, which means it is up to the stat's own
default where to go.  You'll want to create aggregates (summing over
trials at the epoch level, and keeping track of the latest value at
the overall Train process level), so leave the Create Aggregates
toggle checked.  Click Ok.

   Now you get to answer another question: What aggregation operator
do you want for the next (epoch) level?  The default `LAST' (keep
track of just the last value) is not what we need; you want to `SUM'
this stat at the next level, so select `SUM', and then click Ok.

   Note that the stat is also being aggregated at the levels above the
epoch (i.e. the Train level).  It will keep track of the last value
at the Train level.  Note that at each level each statistic has a
name refelcting the process and the nature of the aggregation.  So the
Train level stat is called "lst_sum_Train_0_SE_Stat" indicating it is
the last value of the sum of an SE_Stat and that it is attached to
the Train level.  We have now re-created the squared-error statistic.
You could have skipped over this process, but now you know how to do
this in the future.

   Finally, you need to set the stopping criterion for learning,
which is not set by default.  You can do this by editing the
"Train_0_SE_Stat".  You find it by right-clicking (or selecting and
hitting Edit) the object in the project view, or by doing .processes
/ Edit / Train_0 / lst_sum_Train_0_SE_Stat.  Once this pops up you
will find the `se' value of this stat near the bottom of the window.
This field has 1 parameter `val' which indicates the current value of
the statistic, and 4 parameters which control when the stopping
actually occurs. To set the criteria you need to click on the first
stopping paramater, `stopcrit' which should turn on with a checkmark
symbol indicating that we are indeed using this statistic as a
stopping criteria. The default relation `<=' on the next parameter to
the right is appropriate here so ther is no need to adjust its value.
To the right of the relation parameter is the value to which the
statistic is measured against. In our case, the `val' of this SE
statistic is compared with this value to determine if it is less than
or equal to it.  A reasonable value for this parameter is .04 (.01
for each pattern you are testing, summed over patterns). The next
field labeled `cnt' can be used to set a stricter stopping criterion
where the condition must be met `cnt' times in a row instead of just
once.  We will just leave the value at 1. Click Ok when you are done.

   You will probably also want to create a Test process hierarchy.  A
test sweeping through all of your patterns is an EpochProcess.  So,
select .processes / New / EpochProcess. After you click OK (use right
mouse button to edit) in the popup window, go ahead and edit this
process (if you didn't use right-click, select .processes / Edit /
Epoch_1).  Set `wt_update' to `TEST' and `order' to `SEQUENTIAL'.
Click Ok.  Note that a default SE_Stat was automatically created for
your test process.  There is no need to set any stopping criteria.

   Now, you probably want to monitor some aspect of network function
in your test.  Let's look at the activation of the output units.  To
do this:

   * Select the units in the output layer in the NetView (either the
     whole layer, or all the units in the layer).

   * Select act to display on the NetView.

   * Click Monitor Values / New (located in the left-middle of the
     NetView display).

   * In the pop-up, set the process to "Trial_1".  You'll find it
     under BpTrial when you click on the `In Process' value field.
     Everything else is as we want it so click Ok.

   Also, it is useful to create an EpochCounterStat for the test
process by selecting .processes / NewStat / EpochCounterStat, then,
when the pop-up appears, for In Process, select Trial_1, then click
OK (or select loop_stats under the Trial_1 object in the project
view, and then pick EpochCounterStat from the dialog).  When the
program asks you how to aggregate this statistic, it will remember
the value you entered last time (`SUM'), but we will want to use
`LAST' this time. This will allow you to record information about how
many epochs the network has been trained on each time you run a test.

   Now, so you'll be able to run the net and test it, you can create
control panels for training and testing.  In the project view, select
the Train_0 object and hit Ctrl Panel, and then the same for Epoch_1,
or do .processes / Control Panel / Train_0 for the training panel and
... / Epoch_1 for the testing panel.  In the Train_0 control panel,
you might set the `step' `n' field to something like 30, so that when
you click step it will run for 30 epochs.


File: pdp-user,  Node: tut-config-logs,  Next: tut-config-misc,  Prev: tut-config-processes,  Up: tut-config

Creating Logs For Viewing Data
------------------------------

   Before you run the network, you need to create logs of the results
of training and testing and views that allow you to examine the
contents of these logs.  We'll create a Log with a GraphLogView to
follow the SE_Stat over epochs and Log with a GridLogView to follow
the activations of the output units at test.

   To create the GraphLogView, select .logs / New / GraphLog, and hit
Ok in the New dialog.  It will automatically prompt you for which
process you want to update this view, which should be Epoch_0.  This
will cause the GraphLogView to get information from this process
about what is being monitored there: the SE_Stat, summed across
output units and across patterns in the epoch.  By default the epoch
number will be used at the X axis, so this GraphLogView is now ready.

   To create the GridLogView, select .logs / New / GridLog, and
specify that Trial_1 should update this log.  Again, the header for
the data that are being monitored is automatically retrieved.

   As an advanced example of setting up a grid log, we now describe
how you may be able to set up a special GridLog associated with your
Test process (Epoch_1) that displays the weights in your network at
the end of each test that you run.  This may or may not be something
you really need to do depending on your goals.  But the example shows
some advanced features that are useful and powerful, so we go through
it to expose you to them.

   1a. Create a monitor stat for each projection.  Make sure r.wt is
the current variable displayed, and that you are in `Select' mode
instead of `View' mode. Now select the projection in the netview
which connects the hidden and output layers. Then click Monitor
Values / new.  For In Process, select EpochProcess / Epoch_1, and for
Loop/Final select FINAL, then OK. We selected Final here since we
want to log the values of the weight at the end of each epoch.

   1b. Repeat step 1 selecting the input->hidden layer projection
instead.

   2. Create a new GridLog with .logs / New / GridLog, then OK the
popup, and select Epoch_1 as the updater for this log.

   3. Now comes the interesting part.  We're going to re-arrange this
display so it displays the weights in a way that better reflects the
network's structure.  To do this we are going to change the geometry
and layout of the weight matrices for each projection.  In the
GridLog there is a "header" at the top associated with each column in
the log (the sum-squared error, the EpochCounterStat (labeled epoch)
and each of the two projections you are monitoring, each labeled wt).
We can manipulate these headers with the mouse to rearrange their
layout.

   3a. Use the middle button (or shift plus left button) to reshape
the layout.  We first want to reshape the hidden-to-output weights to
be 2 units wide by 4 units tall, so that each row of units will be the
weights from one of the four output units.  Move the mouse to the
right hand side of the first wt grid, and middle-click and hold it
down while dragging upwards and to the left into a shape that is 4
tall and 2 wide -- it will not let you configure a shape that doesn't
hold all 8 of the weight values, so you need to keep that in mind as
you configure.  It may take a few tries to get this right.

   3b. Next we want to reshape the input-to-hidden weights to be 4
units wide by 2 tall, so that gain one row represents the weights for
one receiving unit.   Do this using the right mouse button.  You can
also use the left mouse button to move the columns around, and if you
want to relabel the columns, that can be done by right-mouse-button
clicking on the headers.

   4. Then, press Run in the test epoch process control panel to see
the weights!  To verify the weigh values and understand how they
correspond to those in the NetView, click on r.wt in the NetView,
(make sure you are in View mode), and select the first hidden unit.
The weights from the input units should be the same as those in the
GridLog for the first row of the bottom set of weights.  Similarly,
the second hidden unit's weights are those in the second row of the
bottom set of weights.  The next set of weights are best viewed using
s.wt to look at the _sending_ weights from the hidden units to the
output units.  The sending weights for the first hidden unit are shown
in the top row of the top set of weights in the GridLog.  Those for
the second hidden unit are in the second row.  This will all be much
clearer in a fully trained network!


File: pdp-user,  Node: tut-config-misc,  Next: tut-config-running,  Prev: tut-config-logs,  Up: tut-config

A Few Miscellaneous Things
--------------------------

   Before you actually run the project, we'll mention a couple of
final things you will want to do.

   You can watch the state of the network in the NetView while it
learns and during testing.  The NetView is automatically updated by
the Trial processes in both the training and testing process
hierarchies.  (You can have it updated by other processes by
selecting View:Object / Add Updater / <process> in the NetView, or by
clicking on a process and a network in the project view, and hitting
Add Updater).  Both of these Trial processes will send the NetView an
update signal, so you can see the values of whatever state variable
you'd like to look at updated after each trial of training or
testing.  You might select `act' as the variable to display in the
NetView, toggle `Auto Range' off, and set the `max' and `min' on the
color scale to 1 and -1.  That way the meanings of the color in the
color scale stay fixed as the various patterns are presented.

   If you are recording the project using a script, you might want to
turn off the scripting process at this point, since the network
creation process is complete.  De-iconify the script and click
StopRec.  You may Edit your script (which should pull up an editor
and allow you to view the script file), to see what steps were taken,
etc.

   Regardless of whether you recorded a script, you will want to save
the state of the project as it is now, so that it can simply be
re-loaded from a project file, just like the XOR example.  Select
Object / Save As in the Project window, and specify a file name for
saving (a ".proj.gz" will automatically be added to the end of the
file name).


File: pdp-user,  Node: tut-config-running,  Prev: tut-config-misc,  Up: tut-config

Training and Testing Your Network
---------------------------------

   Finally, you are ready to run your project.

   Now you can run through a Test -- just hit Run in the Epoch_1
control panel.  You will see the four patterns flicker through the
NetView, and you will see them logged in the GridLogView.  The
outputs all look pretty much the same, at this point.  But now you can
start to train the network.  Click on Step or Run in the Train
control panel, and you are off and running.

   You can play around with parameters, etc. as described in the
previous tutorial on running XOR.

   There is one very important aspect of PDP++ which has not yet been
demonstrated in the tutorial, which we can step through at this point.
This is the ability to apply different parameters to different parts
of the network.  For the purposes of demonstration, we will assume
that we want to have one learning rate for the input-to-hidden
weights, and another learning rate for the hidden-to-output weights.
The basic idea about how to do this is to create two different
ConSpec objects, one of which applies to the input-to-hidden weights,
and another of which applies to the hidden-to-output weights.  An
important facility in PDP++ is the ability to create a _child_ spec
that automatically inherits a set of parameters from a _parent_ spec,
and has a set of unique parameters.  Thus, for the present example,
we would want to create a child ConSpec that inherits everything from
the parent except for the learning rate, which will be unique to it.
Thus, we will have two conspecs that are identical except for the
learning rate.   The advantage of this setup is that if you should
decide to manipulate other parameters such as momentum, weight decay,
etc, the two specs will automatically get the same values of these
changed parameters.

   To create the new child ConSpec, locate the Project window, and do
.specs / New Child / BpConSpec_0 (note that New Child is at the
bottom of the menu).  This will bring up a New object dialog,
specifying that the new BpConSpec will be created as a child of the
existing one.  Select Ok with the right mouse button, so that the new
child will be edited.  Then, click the mouse into the lrate field,
and enter a new learning rate parameter (e.g., .01).  Notice that the
little check-box next to this field was checked when we clicked in
lrate.  This indicates that this is a _unique_ parameter, while the
other, non-checked boxes are _inherited_ parameters.  To see how this
works, let's edit the parent spec.  At the top of the edit dialog,
select Actions / Find Parent, which will bring up the edit dialog for
the parent con spec.  Notice that the parent does not have any of the
unique check boxes, since it does not inherit from anything else.
Now, change another parameter, like momentum, in the parent dialog,
and press Apply.  The Revert button on the child spec is highlighted,
so press it.  Notice that the child dialog displays the new (changed)
momentum value, but retains the unique learning rate parameter
entered before.  Elaborate hierarchies of specs can be created, and
the patterns of inheritence and unique parameters provides a very
clear indication what is specialized about specific spec relative to
the others, and what it has in common.

   Before closing the edit dialogs, it is a good idea to label them
with mnemonic names - call the parent "input-to-hidden cons" and the
child "hidden-to-output cons", for example.  Then click Ok on both
edit dialogs.

   Having created the specs with different learning rates, we now
need to specify that one spec applies to one set of connections, and
another applies to the other set.  As it is now, the parent spec
applies to all projections, since it is the default that was created
automatically when the connections were created.  The best way to set
specs is using the NetView, which has a Selections menu that operates
on selected items in the NetView (it is on the view half (right side)
of the window, since it operates on the items selected in a
particular view).  Thus, make sure you are in Select mode, and select
the projection arrow that goes from the hidden to the output layer.
Then, select Selections / Set Con Spec.  This will bring up a popup
dialog, where you should select BpConSpec / hidden-to-output cons,
and click Ok.  This has now set the selected projection to use this
con spec instead of the default.  To verify the status of the
network, click in the backround of the NetView to unselect
everything, and then choose Selections / Show Con Spec.  In the
dialog, select hidden-to-output cons, and click Ok.  The correct
projection arrow will be selected in the NetView, indicating that it
is using that con spec.  You can repeat this to verify that the other
projection is still using "input-to-hidden cons".  Thus, the
Selections menu allows you to both set and verify which objects in
the network are using which specs.  Notice that you can set many
other properties using this menu much in the same way we just did for
ConSpecs on projections.

   At this point, you might want to look at some of the other demo
projects available.  These are located in the `demo' directory where
the XOR example project was.  Check out the `README' files for further
information.


File: pdp-user,  Node: how,  Next: gui,  Prev: tut,  Up: Top

How-to Guide
************

   This chapter provides step-by-step instructions on how to perform
various commonly-used operations.  It contains pointers to the other
sections of the manual that provide more information about each step
of the operation, so look for something close to what you want to do
if you can't find it exactly.  Also, many questions can be answered
by looking at the relevant section of the manual.

   It also contains a description of the `Wizard' object, which
automates many of these commonly-used tasks.

* Menu:

* how-proc::                    Questions about Processes
* how-net::                     Questions about Networks
* how-env::                     Questions about Environments
* how-css::                     Questions about CSS
* how-wizard::                  The Wizard Object

